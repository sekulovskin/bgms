[{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Model Comparison with bgmCompare","text":"function bgmCompare() extends bgm() independent-sample designs. estimates whether edge weights category thresholds differ across groups ordinal Markov random field (MRF). Posterior inclusion probabilities indicate plausible group difference exists given parameter. can converted Bayes factors hypothesis testing.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"boredom-dataset","dir":"Articles","previous_headings":"","what":"Boredom dataset","title":"Model Comparison with bgmCompare","text":"illustrate subset Boredom dataset included bgms.","code":"library(bgms)  ?Boredom data_french = Boredom[Boredom$language == \"fr\", -1] data_french = data_french[, 1:5] data_english = Boredom[Boredom$language != \"fr\", -1] data_english = data_english[, 1:5]"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"fitting-a-model","dir":"Articles","previous_headings":"","what":"Fitting a model","title":"Model Comparison with bgmCompare","text":"Note: fitting, progress bars shown interactive sessions. vignette, suppressed clarity. Sampling can take ; progress bars usually help track progress.","code":"fit = bgmCompare(x = data_french, y = data_english, seed = 1234)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"posterior-summaries","dir":"Articles","previous_headings":"","what":"Posterior summaries","title":"Model Comparison with bgmCompare","text":"summary shows baseline effects group differences: can extract posterior means inclusion probabilities:","code":"summary(fit) #> Posterior summaries from Bayesian grouped MRF estimation (bgmCompare): #>  #> Category thresholds: #>        parameter    mean  mcse    sd    n_eff  Rhat #> 1 loose_ends (1)  -0.938 0.002 0.096 3415.827 1.001 #> 2 loose_ends (2)  -2.518 0.003 0.139 2069.362 1.004 #> 3 loose_ends (3)  -3.796 0.005 0.183 1543.211 1.005 #> 4 loose_ends (4)  -5.082 0.007 0.243 1236.538 1.006 #> 5 loose_ends (5)  -7.609 0.010 0.335 1164.760 1.007 #> 6 loose_ends (6) -10.106 0.014 0.452 1092.310 1.006 #> ... (use `summary(fit)$main` to see full output) #>  #> Pairwise interactions: #>                parameter  mean mcse    sd    n_eff  Rhat #> 1   loose_ends-entertain 0.170    0 0.013 2245.500 1.002 #> 2  loose_ends-repetitive 0.058    0 0.012 2059.968 1.007 #> 3 loose_ends-stimulation 0.126    0 0.012 2648.651 1.002 #> 4   loose_ends-motivated 0.141    0 0.013 2490.105 1.002 #> 5   entertain-repetitive 0.067    0 0.012 2712.597 1.000 #> 6  entertain-stimulation 0.109    0 0.012 2591.243 1.001 #> ... (use `summary(fit)$pairwise` to see full output) #>  #> Inclusion probabilities: #>                          parameter  mean    sd  mcse n0->0 n0->1 n1->0 #>                  loose_ends (main) 0.009 0.094 0.003  3947    16    16 #>    loose_ends-entertain (pairwise) 0.020 0.139 0.002  3845    75    75 #>   loose_ends-repetitive (pairwise) 0.507 0.500 0.018  1640   331   330 #>  loose_ends-stimulation (pairwise) 0.042 0.200 0.003  3680   152   152 #>    loose_ends-motivated (pairwise) 0.018 0.132 0.002  3860    68    68 #>                   entertain (main) 0.001 0.039 0.001  3988     5     5 #>  n1->1    n_eff  Rhat #>     20 1156.240 1.027 #>      4 3755.694 1.001 #>   1698  792.304 1.002 #>     15 3617.928 1.012 #>      3 3805.320 1.004 #>      1 2864.511 1.081 #> ... (use `summary(fit)$indicator` to see full output) #> Note: NA values are suppressed in the print table. They occur when an indicator #> was constant (all 0 or all 1) across all iterations, so sd/mcse/n_eff/Rhat #> are undefined; `summary(fit)$indicator` still contains the NA values. #>  #> Group differences (main effects): #>              parameter   mean    sd  mcse    n_eff  Rhat #>  loose_ends (diff1; 1)  0.000 0.002 0.000   47.557 1.000 #>  loose_ends (diff1; 2)  0.004 0.044 0.001 1082.785 1.001 #>  loose_ends (diff1; 3)  0.004 0.043 0.001  902.638 1.001 #>  loose_ends (diff1; 4)  0.004 0.042 0.001  923.443 1.001 #>  loose_ends (diff1; 5)  0.001 0.010 0.001  113.681 1.000 #>  loose_ends (diff1; 6) -0.003 0.031 0.001  630.010 1.000 #> ... (use `summary(fit)$main_diff` to see full output) #> Note: NA values are suppressed in the print table. They occur here when an #> indicator was zero across all iterations, so mcse/n_eff/Rhat are undefined; #> `summary(fit)$main_diff` still contains the NA values. #>  #> Group differences (pairwise effects): #>                       parameter  mean    sd  mcse    n_eff  Rhat #>    loose_ends-entertain (diff1) 0.000 0.000 0.000   92.057 1.001 #>   loose_ends-repetitive (diff1) 0.021 0.022 0.001  837.278 1.001 #>  loose_ends-stimulation (diff1) 0.001 0.005 0.000 2789.970 1.001 #>    loose_ends-motivated (diff1) 0.000 0.002 0.000 1517.163 1.000 #>    entertain-repetitive (diff1) 0.004 0.011 0.000 1174.633 1.001 #>   entertain-stimulation (diff1) 0.002 0.007 0.000 2567.839 1.001 #> ... (use `summary(fit)$pairwise_diff` to see full output) #> Note: NA values are suppressed in the print table. They occur here when an #> indicator was zero across all iterations, so mcse/n_eff/Rhat are undefined; #> `summary(fit)$pairwise_diff` still contains the NA values. #>  #> Use `summary(fit)$<component>` to access full results. #> See the `easybgm` package for other summary and plotting tools. coef(fit) #> $main_effects_raw #>                    baseline         diff1 #> loose_ends(c1)   -0.9376669 -8.550637e-05 #> loose_ends(c2)   -2.5179548  4.207447e-03 #> loose_ends(c3)   -3.7960694  4.109076e-03 #> loose_ends(c4)   -5.0822845  4.002782e-03 #> loose_ends(c5)   -7.6089748  9.141143e-04 #> loose_ends(c6)  -10.1063777 -2.905428e-03 #> entertain(c1)    -0.8692630 -4.662515e-04 #> entertain(c2)    -2.2458877 -1.496695e-04 #> entertain(c3)    -3.8177841  2.483365e-04 #> entertain(c4)    -5.1697251 -3.493918e-04 #> entertain(c5)    -7.0455504 -2.961615e-05 #> entertain(c6)    -9.5780234  8.034366e-04 #> repetitive(c1)   -0.1379151 -8.806321e-03 #> repetitive(c2)   -0.6611929 -1.585117e-02 #> repetitive(c3)   -1.0866300 -5.037304e-03 #> repetitive(c4)   -1.8627494  6.470289e-03 #> repetitive(c5)   -3.2512960  2.020118e-02 #> repetitive(c6)   -5.0299108  2.038924e-02 #> stimulation(c1)  -0.5457673 -3.167586e-03 #> stimulation(c2)  -1.7894587 -1.004249e-03 #> stimulation(c3)  -2.5304682 -1.332632e-03 #> stimulation(c4)  -3.6190432 -2.226982e-03 #> stimulation(c5)  -5.0975023 -1.200563e-03 #> stimulation(c6)  -7.0661001 -3.494762e-03 #> motivated(c1)    -0.5585283 -2.017340e-03 #> motivated(c2)    -1.8120817 -1.070582e-03 #> motivated(c3)    -3.2996542  1.534816e-03 #> motivated(c4)    -4.7945890  2.748786e-03 #> motivated(c5)    -6.7933940 -6.137974e-04 #> motivated(c6)    -9.1663037  1.980850e-03 #>  #> $pairwise_effects_raw #>                          baseline         diff1 #> loose_ends-entertain   0.16991838  2.642296e-05 #> loose_ends-repetitive  0.05811138  2.137907e-02 #> loose_ends-stimulation 0.12622492  9.635558e-04 #> loose_ends-motivated   0.14097684 -2.398129e-04 #> entertain-repetitive   0.06659031  4.431347e-03 #> entertain-stimulation  0.10937128  1.801550e-03 #> entertain-motivated    0.08666756  3.824078e-03 #> repetitive-stimulation 0.05631314  6.388122e-04 #> repetitive-motivated   0.13750770  1.397853e-02 #> stimulation-motivated  0.10798919  1.154900e-04 #>  #> $main_effects_groups #>                      group1      group2 #> loose_ends(c1)   -0.9376242  -0.9377097 #> loose_ends(c2)   -2.5200585  -2.5158511 #> loose_ends(c3)   -3.7981240  -3.7940149 #> loose_ends(c4)   -5.0842859  -5.0802831 #> loose_ends(c5)   -7.6094318  -7.6085177 #> loose_ends(c6)  -10.1049250 -10.1078305 #> entertain(c1)    -0.8690299  -0.8694961 #> entertain(c2)    -2.2458129  -2.2459625 #> entertain(c3)    -3.8179083  -3.8176599 #> entertain(c4)    -5.1695504  -5.1698998 #> entertain(c5)    -7.0455356  -7.0455652 #> entertain(c6)    -9.5784251  -9.5776217 #> repetitive(c1)   -0.1335119  -0.1423182 #> repetitive(c2)   -0.6532674  -0.6691185 #> repetitive(c3)   -1.0841114  -1.0891487 #> repetitive(c4)   -1.8659846  -1.8595143 #> repetitive(c5)   -3.2613966  -3.2411954 #> repetitive(c6)   -5.0401054  -5.0197162 #> stimulation(c1)  -0.5441836  -0.5473511 #> stimulation(c2)  -1.7889565  -1.7899608 #> stimulation(c3)  -2.5298019  -2.5311345 #> stimulation(c4)  -3.6179297  -3.6201567 #> stimulation(c5)  -5.0969021  -5.0981026 #> stimulation(c6)  -7.0643527  -7.0678475 #> motivated(c1)    -0.5575196  -0.5595369 #> motivated(c2)    -1.8115464  -1.8126170 #> motivated(c3)    -3.3004216  -3.2988868 #> motivated(c4)    -4.7959634  -4.7932146 #> motivated(c5)    -6.7930871  -6.7937009 #> motivated(c6)    -9.1672942  -9.1653133 #>  #> $pairwise_effects_groups #>                            group1     group2 #> loose_ends-entertain   0.16990517 0.16993159 #> loose_ends-repetitive  0.04742184 0.06880091 #> loose_ends-stimulation 0.12574314 0.12670670 #> loose_ends-motivated   0.14109674 0.14085693 #> entertain-repetitive   0.06437464 0.06880598 #> entertain-stimulation  0.10847051 0.11027206 #> entertain-motivated    0.08475552 0.08857960 #> repetitive-stimulation 0.05599374 0.05663255 #> repetitive-motivated   0.13051843 0.14449696 #> stimulation-motivated  0.10793144 0.10804693 #>  #> $indicators #>             loose_ends entertain repetitive stimulation motivated #> loose_ends     0.00900   0.01975    0.50725     0.04175   0.01775 #> entertain      0.01975   0.00150    0.13725     0.06650   0.11425 #> repetitive     0.50725   0.13725    0.03675     0.03175   0.36100 #> stimulation    0.04175   0.06650    0.03175     0.00575   0.01525 #> motivated      0.01775   0.11425    0.36100     0.01525   0.00700"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"visualizing-group-networks","dir":"Articles","previous_headings":"","what":"Visualizing group networks","title":"Model Comparison with bgmCompare","text":"can use output plot network French sample:","code":"library(qgraph)  french_network = matrix(0, 5, 5) french_network[lower.tri(french_network)] = coef(fit)$pairwise_effects_groups[, 1] french_network = french_network + t(french_network) colnames(french_network) = colnames(data_french) rownames(french_network) = colnames(data_french)  qgraph(french_network,        theme = \"TeamFortress\",        maximum = 1,        fade = FALSE,        color = c(\"#f0ae0e\"), vsize = 10, repulsion = .9,        label.cex = 1, label.scale = \"FALSE\",        labels = colnames(data_french))"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Model Comparison with bgmCompare","text":"one-sample analysis, see Getting Started vignette. diagnostics convergence checks, see Diagnostics vignette. additional analysis tools advanced plotting options, consider using easybgm package, integrates smoothly bgms objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Diagnostics and Spike-and-Slab Summaries","text":"vignette illustrates inspect convergence diagnostics interpret spike--slab summaries bgms models. model variables spike--slab priors introduce binary indicator variables govern whether effect included . posterior distributions can summarized inclusion probabilities Bayes factors.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"example-fit","dir":"Articles","previous_headings":"","what":"Example fit","title":"Diagnostics and Spike-and-Slab Summaries","text":"use subset Wenchuan dataset: Note: fitting, progress bars shown interactive sessions. vignette, suppressed clarity. Sampling can take ; progress bars usually help track progress.","code":"library(bgms) data = Wenchuan[, 1:5] fit = bgm(data, seed = 1234)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"convergence-diagnostics","dir":"Articles","previous_headings":"","what":"Convergence diagnostics","title":"Diagnostics and Spike-and-Slab Summaries","text":"quality Markov chain can assessed common MCMC diagnostics: R-hat values close 1 (typically 1.01) suggest convergence (Vehtari et al., 2021). effective sample size (ESS) reflects number independent samples provide equivalent precision. Larger ESS values indicate reliable estimates. Monte Carlo standard error (MCSE) measures additional variability introduced using finite number MCMC draws. small MCSE relative posterior standard deviation indicates stable estimates, whereas large MCSE suggests samples needed. Advanced users can inspect traceplots extracting raw samples using external packages coda bayesplot. example using coda package create traceplot pairwise effect parameter.","code":"summary(fit)$pairwise #>                          mean          sd        mcse     n_eff #> intrusion-dreams  0.631924323 0.001551325 0.064266150 1716.1643 #> intrusion-flash   0.338297975 0.001448421 0.061854243 1823.6826 #> intrusion-upset   0.190695963 0.076279341 0.005921381  165.9464 #> intrusion-physior 0.198176315 0.065555747 0.003647515  323.0185 #> dreams-flash      0.498040695 0.001270565 0.060466106 2264.8025 #> dreams-upset      0.230776860 0.056205053 0.002092867  721.2197 #> dreams-physior    0.005254907 0.021861327 0.000843593  671.5636 #> flash-upset       0.006462176 0.024675968 0.001036232  567.0671 #> flash-physior     0.307138582 0.001205035 0.053371694 1961.6559 #> upset-physior     0.707867139 0.001478310 0.059720575 1631.9869 #>                        Rhat #> intrusion-dreams  0.9998582 #> intrusion-flash   0.9999985 #> intrusion-upset   1.0325834 #> intrusion-physior 1.0053426 #> dreams-flash      0.9999931 #> dreams-upset      1.0076821 #> dreams-physior    1.0051407 #> flash-upset       1.0115874 #> flash-physior     1.0013808 #> upset-physior     0.9999992 library(coda)  param_index = 1 chains = lapply(fit$raw_samples$pairwise, function(mat) mat[, param_index]) mcmc_obj = mcmc.list(lapply(chains, mcmc))  traceplot(mcmc_obj, col = c(\"firebrick\", \"steelblue\", \"darkgreen\", \"goldenrod\"),            main = \"Traceplot of pairwise[1]\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"spike-and-slab-summaries","dir":"Articles","previous_headings":"","what":"Spike-and-slab summaries","title":"Diagnostics and Spike-and-Slab Summaries","text":"spike--slab prior yields posterior inclusion probabilities edges: Values near 1.0: strong evidence edge present. Values near 0.0: strong evidence edge absent. Values near 0.5: inconclusive (absence evidence).","code":"coef(fit)$indicator #>           intrusion  dreams   flash   upset physior #> intrusion   0.00000 1.00000 1.00000 0.96575  0.0560 #> dreams      1.00000 0.00000 0.92525 1.00000  0.0655 #> flash       1.00000 0.92525 0.00000 0.99600  1.0000 #> upset       0.96575 1.00000 0.99600 0.00000  1.0000 #> physior     0.05600 0.06550 1.00000 1.00000  0.0000"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"bayes-factors","dir":"Articles","previous_headings":"","what":"Bayes factors","title":"Diagnostics and Spike-and-Slab Summaries","text":"prior inclusion probability edge equal 0.5 (e.g., using Bernoulli prior inclusion_probability = 0.5 symmetric Beta prior, main_alpha = main_beta), can directly transform inclusion probabilities Bayes factors edge presence vs absence: Bayes factor favor inclusion (H1) small, meaning little evidence inclusion. Since Bayes factor transitive, can use express evidence favor exclusion (H0) Bayes factor shows strong evidence absence network relation variables intrusion physior.","code":"# Example for one edge p = coef(fit)$indicator[1, 5] BF_10 = p / (1 - p) BF_10 #> [1] 0.05932203 1 / BF_10 #> [1] 16.85714"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"notes-on-runtime","dir":"Articles","previous_headings":"","what":"Notes on runtime","title":"Diagnostics and Spike-and-Slab Summaries","text":"Sampling spike--slab priors can take longer. interactive sessions, progress bars displayed. vignette, suppressed readability.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Diagnostics and Spike-and-Slab Summaries","text":"See Getting Started simple one-sample workflow. See Model Comparison group differences.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with bgms","text":"bgms package implements Bayesian methods analyzing graphical models binary ordinal variables. estimates main effects (category thresholds) pairwise interactions ordinal Markov random field (MRF), optional Bayesian edge selection via spike––slab priors. package provides two main entry points: bgm() one-sample designs (single network), bgmCompare() independent-sample designs (group comparisons). vignette walks basic workflow: fitting model, summarizing posterior output, visualizing results.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"wenchuan-dataset","dir":"Articles","previous_headings":"","what":"Wenchuan dataset","title":"Getting Started with bgms","text":"dataset Wenchuan contains responses survivors 2008 Wenchuan earthquake posttraumatic stress items. , analyze subset first five items demonstration.","code":"library(bgms)  # Analyse a subset of the Wenchuan dataset ?Wenchuan data = Wenchuan[, 1:5] head(data) #>      intrusion dreams flash upset physior #> [1,]         2      2     2     2       3 #> [2,]         2      2     2     3       3 #> [3,]         2      4     4     4       3 #> [4,]         2      1     2     2       1 #> [5,]         2      2     2     2       2 #> [6,]         4      3     2     2       2"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"fitting-a-model","dir":"Articles","previous_headings":"","what":"Fitting a model","title":"Getting Started with bgms","text":"main entry point bgm() single-group models bgmCompare() multiple-group comparisons. Note: fitting, progress bars shown interactive sessions. vignette, suppressed clarity. Sampling can take ; progress bars usually help track progress.","code":"fit = bgm(data, seed = 1234)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"posterior-summaries","dir":"Articles","previous_headings":"","what":"Posterior summaries","title":"Getting Started with bgms","text":"can also access posterior means inclusion probabilities directly:","code":"summary(fit) #> Posterior summaries from Bayesian estimation: #>  #> Category thresholds: #>                 mean  mcse    sd    n_eff  Rhat #> intrusion (1)  0.489 0.006 0.233 1596.771 1.010 #> intrusion (2) -1.888 0.010 0.340 1053.243 1.024 #> intrusion (3) -4.828 0.019 0.556  827.554 1.025 #> intrusion (4) -9.490 0.031 0.897  860.034 1.023 #> dreams (1)    -0.597 0.005 0.190 1610.037 1.002 #> dreams (2)    -3.802 0.010 0.341 1174.409 1.003 #> ... (use `summary(fit)$main` to see full output) #>  #> Pairwise interactions: #>                    mean    sd  mcse    n_eff  Rhat #> intrusion-dreams  0.632 0.002 0.064 1716.164 1.000 #> intrusion-flash   0.338 0.001 0.062 1823.683 1.000 #> intrusion-upset   0.191 0.076 0.006  165.946 1.033 #> intrusion-physior 0.198 0.066 0.004  323.018 1.005 #> dreams-flash      0.498 0.001 0.060 2264.802 1.000 #> dreams-upset      0.231 0.056 0.002  721.220 1.008 #> ... (use `summary(fit)$pairwise` to see full output) #> Note: NA values are suppressed in the print table. They occur here when an  #> indicator was zero across all iterations, so mcse/n_eff/Rhat are undefined; #> `summary(fit)$pairwise` still contains the NA values. #>  #> Inclusion probabilities: #>                    mean    sd  mcse n0->0 n0->1 n1->0 n1->1   n_eff #> intrusion-dreams  1.000 0.000           0     0     0  3999         #> intrusion-flash   1.000 0.000           0     0     0  3999         #> intrusion-upset   0.925 0.263 0.028   287    12    12  3688  88.677 #> intrusion-physior 0.966 0.182 0.016   129     8     8  3854 124.701 #> dreams-flash      1.000 0.000           0     0     0  3999         #> dreams-upset      0.996 0.063 0.004    14     2     2  3981  267.81 #>                    Rhat #> intrusion-dreams        #> intrusion-flash         #> intrusion-upset   1.173 #> intrusion-physior 1.059 #> dreams-flash            #> dreams-upset        1.3 #> ... (use `summary(fit)$indicator` to see full output) #> Note: NA values are suppressed in the print table. They occur when an indicator #> was constant (all 0 or all 1) across all iterations, so sd/mcse/n_eff/Rhat #> are undefined; `summary(fit)$indicator` still contains the NA values. #>  #> Use `summary(fit)$<component>` to access full results. #> See the `easybgm` package for other summary and plotting tools. coef(fit) #> $main #>              cat (1)   cat (2)   cat (3)    cat (4) #> intrusion  0.4889848 -1.887789 -4.827802  -9.490176 #> dreams    -0.5974490 -3.801634 -7.133124 -11.578501 #> flash     -0.1063310 -2.567018 -5.359779  -9.659237 #> upset      0.4144289 -1.303246 -3.365035  -7.015933 #> physior   -0.6147997 -3.170385 -6.211313 -10.550982 #>  #> $pairwise #>           intrusion      dreams       flash       upset     physior #> intrusion 0.0000000 0.631924323 0.338297975 0.190695963 0.198176315 #> dreams    0.6319243 0.000000000 0.498040695 0.230776860 0.005254907 #> flash     0.3382980 0.498040695 0.000000000 0.006462176 0.307138582 #> upset     0.1906960 0.230776860 0.006462176 0.000000000 0.707867139 #> physior   0.1981763 0.005254907 0.307138582 0.707867139 0.000000000 #>  #> $indicator #>           intrusion  dreams   flash   upset physior #> intrusion   0.00000 1.00000 1.00000 0.96575  0.0560 #> dreams      1.00000 0.00000 0.92525 1.00000  0.0655 #> flash       1.00000 0.92525 0.00000 0.99600  1.0000 #> upset       0.96575 1.00000 0.99600 0.00000  1.0000 #> physior     0.05600 0.06550 1.00000 1.00000  0.0000"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"network-plot","dir":"Articles","previous_headings":"","what":"Network plot","title":"Getting Started with bgms","text":"visualize network structure, threshold posterior inclusion probabilities 0.5 plot resulting adjacency matrix.","code":"library(qgraph)  median_probability_network = coef(fit)$pairwise median_probability_network[coef(fit)$indicator < 0.5] = 0.0  qgraph(median_probability_network,         theme = \"TeamFortress\",         maximum = 1,        fade = FALSE,        color = c(\"#f0ae0e\"), vsize = 10, repulsion = .9,         label.cex = 1, label.scale = \"FALSE\",         labels = colnames(data))"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Getting Started with bgms","text":"comparing groups, see ?bgmCompare Model Comparison vignette. diagnostics convergence checks, see Diagnostics vignette.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Maarten Marsman. Author, maintainer. Giuseppe Arena. Contributor. Karoline Huth. Contributor. Nikola Sekulovski. Contributor. Don van den Bergh. Contributor.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Marsman, M., Arena, G., Huth, K., Sekulovski, N., & van den Bergh, D. (2025). bgms: Bayesian analysis networks binary /ordinal variables. https://CRAN.R-project.org/package=bgms","code":"@Manual{,   title = {bgms: Bayesian analysis of networks of binary and/or ordinal variables},   author = {Maarten Marsman and Giuseppe Arena and Karoline Huth and Nikola Sekulovski and Don {van den Bergh}},   year = {2025},   url = {https://CRAN.R-project.org/package=bgms},   note = {R package version 0.1.6.2}, }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"bgms-","dir":"","previous_headings":"","what":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"Bayesian analysis graphical models binary ordinal variables bgms package implements Bayesian estimation model comparison ordinal Markov random fields (MRFs), graphical models represent networks binary /ordinal variables (Marsman et al., 2025). likelihood approximated pseudolikelihood, Markov chain Monte Carlo (MCMC) methods used sample corresponding pseudoposterior distribution model parameters.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"main-functions","dir":"","previous_headings":"","what":"Main functions","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"package two main entry points: bgm() – estimates single network one-sample design. bgmCompare() – compares networks groups independent-sample design.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"effect-selection","dir":"","previous_headings":"","what":"Effect selection","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"functions support effect selection spike--slab priors: Edges one-sample designs:bgm() models presence absence edges variables. Posterior inclusion probabilities indicate plausibility edge can converted Bayes factors conditional independence tests (see Marsman et al., 2025; Sekulovski et al., 2024). Communities/clusters one-sample designs:bgm() can also model community structure. Posterior probabilities number clusters quantify plausibility clustering solutions can converted Bayes factors (see Sekulovski et al., 2025). Group differences independent-sample designs:bgmCompare() models differences edge weights category thresholds groups. Posterior inclusion probabilities indicate plausibility parameter differences can converted Bayes factors tests parameter equivalence (see Marsman et al., 2024).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn more","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"worked examples tutorials, see package vignettes: Getting Started Model Comparison Diagnostics Spike--Slab Summaries can also access directly R :","code":"browseVignettes(\"bgms\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"why-use-markov-random-fields","dir":"","previous_headings":"","what":"Why use Markov Random Fields?","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"Graphical models networks become central recent psychological psychometric research (Contreras et al., 2019; Marsman & Rhemtulla, 2022; Robinaugh et al., 2020). Markov random field (MRF) models, graph structure reflects partial associations variables (Kindermann & Snell, 1980). MRF, missing edge two variables implies conditional independence given rest network (Lauritzen, 2004). words, remaining variables fully explain away potential association unconnected pair.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"why-use-a-bayesian-approach","dir":"","previous_headings":"","what":"Why use a Bayesian approach?","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"analyzing MRF, often want compare competing hypotheses: Edge presence vs. edge absence (conditional dependence vs. independence) one-sample designs. Parameter difference vs. parameter equivalence independent-sample designs. Frequentist approaches limited comparisons: can reject null hypothesis, provide evidence . result, edge difference excluded, remains unclear whether reflects true absence simply insufficient power. Bayesian inference avoids problem. Using inclusion Bayes factors (Huth et al., 2023; Sekulovski et al., 2024), can quantify evidence directions: Evidence edge presence vs. evidence edge absence, Evidence parameter difference vs. evidence parameter equivalence. makes possible detect structure group differences, also conclude absence evidence.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"current developmental version can installed ","code":"if (!requireNamespace(\"remotes\")) {    install.packages(\"remotes\")    }    remotes::install_github(\"Bayesian-Graphical-Modelling-Lab/bgms\")"},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":null,"dir":"Reference","previous_headings":"","what":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"dataset includes ADHD symptom ratings 355 children aged 6 8 years Children’s Attention Project (CAP) cohort (Silk et al. 2019) . sample consists 146 children diagnosed ADHD 209 without diagnosis. Symptoms assessed structured interviews parents using NIMH Diagnostic Interview Schedule Children IV (DISC-IV) (Shaffer et al. 2000) . checklist includes 18 items: 9 Inattentive () 9 Hyperactive/Impulsive (HI). item binary (1 = present, 0 = absent).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"","code":"data(\"ADHD\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"matrix 355 rows 19 columns. group ADHD diagnosis: 1 = diagnosed, 0 = diagnosed avoid Often avoids, dislikes, reluctant engage tasks   require sustained mental effort () closeatt Often fails give close attention details makes   careless mistakes schoolwork, work, activities () distract often easily distracted extraneous stimuli () forget often forgetful daily activities () instruct Often follow instructions fails   finish schoolwork, chores, duties workplace () listen Often seem listen spoken directly   () loses Often loses things necessary tasks activities () org Often difficulty organizing tasks activities () susatt Often difficulty sustaining attention tasks play   activities () blurts Often blurts answers questions completed   (HI) fidget Often fidgets hands feet squirms seat   (HI) interrupt Often interrupts intrudes others (HI) motor often \"go\" often acts \"driven motor\"   (HI) quiet Often difficulty playing engaging leisure activities   quietly (HI) runs Often runs climbs excessively situations   inappropriate (HI) seat Often leaves seat classroom situations   remaining seated expected (HI) talks Often talks excessively (HI) turn Often difficulty awaiting turn (HI)","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"Silk et al. (2019) . Data retrieved doi:10.1371/journal.pone.0211053.s004 . Licensed CC-4.0: https://creativecommons.org/licenses//4.0/","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"Shaffer D, Fisher P, Lucas CP, Dulcan MK, Schwab-Stone (2000). “NIMH Diagnostic Interview Schedule Children Version IV (NIMH DISC-IV): description, differences previous versions, reliability common diagnoses.” Journal American Academy Child & Adolescent Psychiatry, 39, 28–38. doi:10.1097/00004583-200001000-00014 , PMID: 10638065. Silk TJ, Malpas CB, Beare R, Efron D, Anderson V, Hazell P, Jongeling B, Nicholson JM, Sciberras E (2019). “network analysis approach ADHD symptoms: sum parts.” PLOS ONE, 14(1), e0211053. doi:10.1371/journal.pone.0211053 .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":null,"dir":"Reference","previous_headings":"","what":"Short Boredom Proneness Scale Responses — Boredom","title":"Short Boredom Proneness Scale Responses — Boredom","text":"dataset includes responses 8-item Short Boredom Proneness Scale (SBPS), self-report measure individual's susceptibility boredom (Martarelli et al. 2023) . Items rated 7-point Likert scale ranging 1 (\"strongly disagree\") 7 (\"strongly agree\"). scale administered either English (Struk et al. 2015)  French (translated (Martarelli et al. 2023) ).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Short Boredom Proneness Scale Responses — Boredom","text":"","code":"data(\"Boredom\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Short Boredom Proneness Scale Responses — Boredom","text":"matrix 986 rows 9 columns. row corresponds respondent. language Language SBPS administered: \"en\" = English, \"fr\" = French loose_ends often find “loose ends,” knowing   . entertain find hard entertain . repetitive Many things repetitive monotonous. stimulation takes stimulation get going   people. motivated feel motivated things . keep_interest situations, hard find   something see keep interested. sit_around Much time, just sit around nothing. half_dead_dull Unless something exciting, even dangerous,   feel half-dead dull.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Short Boredom Proneness Scale Responses — Boredom","text":"Martarelli et al. (2023) . Data retrieved https://osf.io/qhux8. Licensed CC-4.0: https://creativecommons.org/licenses//4.0/","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Short Boredom Proneness Scale Responses — Boredom","text":"Martarelli CS, Baillifard , Audrin C (2023). “Trait-Based Network Perspective Validation French Short Boredom Proneness Scale.” European Journal Psychological Assessment, 39(6), 390–399. doi:10.1027/1015-5759/a000718 . Struk AA, Carriere JSA, Cheyne JA, Danckert J (2015). “Short Boredom Proneness Scale: Development Psychometric Properties.” Assessment, 24(3), 346–359. doi:10.1177/1073191115609996 .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":null,"dir":"Reference","previous_headings":"","what":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"dataset contains responses 17 items assessing symptoms post-traumatic stress disorder (PTSD) Chinese adults survived 2008 Wenchuan earthquake lost least one child disaster (McNally et al. 2015) . Participants completed civilian version Posttraumatic Checklist, item corresponding DSM-IV PTSD symptom. Items rated 5-point Likert scale \"\" \"extremely,\" indicating degree symptom bothered respondent past month.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"","code":"data(\"Wenchuan\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"matrix 362 rows 17 columns. row represents participant. intrusion Repeated, disturbing memories, thoughts, images   stressful experience past? dreams Repeated, disturbing dreams stressful experience   past? flash Suddenly acting feeling stressful experience   happening (reliving )? upset Feeling upset something reminded stressful   experience past? physior physical reactions (e.g., heart pounding, trouble   breathing, sweating) something reminded stressful experience   past? avoidth Avoiding thinking talking stressful   experience past avoiding feelings related ? avoidact Avoiding activities situations reminded   stressful experience past? amnesia Trouble remembering important parts stressful   experience past? lossint Loss interest activities used enjoy? distant Feeling distant cut people? numb Feeling emotionally numb unable loving   feelings close ? future Feeling future somehow cut short? sleep Trouble falling staying asleep? anger Feeling irritable angry outbursts? concen difficulty concentrating? hyper \"super-alert\" watchful guard? startle Feeling jumpy easily startled?","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"https://psychosystems.org/wp-content/uploads/2014/10/Wenchuan.csv","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"McNally RJ, Robinaugh DJ, Wu GWY, Wang L, Deserno MK, Borsboom D (2015). “Mental disorders causal systems: network approach posttraumatic stress disorder.” Clinical Psychological Science, 6, 836–849. doi:10.1177/2167702614553230 .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"bgm function estimates pseudoposterior distribution category thresholds (main effects) pairwise interaction parameters Markov Random Field (MRF) model binary /ordinal variables. Optionally, performs Bayesian edge selection using spike--slab priors infer network structure.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"","code":"bgm(   x,   variable_type = \"ordinal\",   baseline_category,   iter = 1000,   warmup = 1000,   pairwise_scale = 2.5,   main_alpha = 0.5,   main_beta = 0.5,   edge_selection = TRUE,   edge_prior = c(\"Bernoulli\", \"Beta-Bernoulli\", \"Stochastic-Block\"),   inclusion_probability = 0.5,   beta_bernoulli_alpha = 1,   beta_bernoulli_beta = 1,   dirichlet_alpha = 1,   lambda = 1,   na_action = c(\"listwise\", \"impute\"),   update_method = c(\"nuts\", \"adaptive-metropolis\", \"hamiltonian-mc\"),   target_accept,   hmc_num_leapfrogs = 100,   nuts_max_depth = 10,   learn_mass_matrix = FALSE,   chains = 4,   cores = parallel::detectCores(),   display_progress = c(\"per-chain\", \"total\", \"none\"),   seed = NULL,   interaction_scale,   burnin,   save,   threshold_alpha,   threshold_beta )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"x data frame matrix n rows p columns containing binary ordinal responses. Variables automatically recoded non-negative integers (0, 1, ..., m). regular ordinal variables, unobserved categories collapsed; Blume–Capel variables, categories retained. variable_type Character character vector. Specifies type variable x. Allowed values: \"ordinal\" \"blume-capel\". Binary variables automatically treated \"ordinal\". Default: \"ordinal\". baseline_category Integer vector. Baseline category used Blume–Capel variables. Can single integer (applied ) vector length p. Required least one variable type \"blume-capel\". iter Integer. Number post–burn-iterations (per chain). Default: 1e3. warmup Integer. Number warmup iterations collecting samples. minimum 1000 iterations enforced, warning smaller value requested. Default: 1e3. pairwise_scale Double. Scale Cauchy prior pairwise interaction parameters. Default: 2.5. main_alpha, main_beta Double. Shape parameters beta-prime prior threshold parameters. Must positive. equal, prior symmetric. Defaults: main_alpha = 0.5 main_beta = 0.5. edge_selection Logical. Whether perform Bayesian edge selection. FALSE, model estimates edges. Default: TRUE. edge_prior Character. Specifies prior edge inclusion. Options: \"Bernoulli\", \"Beta-Bernoulli\", \"Stochastic-Block\". Default: \"Bernoulli\". inclusion_probability Numeric scalar. Prior inclusion probability edge (used Bernoulli prior). Default: 0.5. beta_bernoulli_alpha, beta_bernoulli_beta Double. Shape parameters beta distribution Beta–Bernoulli Stochastic-Block priors. Must positive. Defaults: beta_bernoulli_alpha = 1 beta_bernoulli_beta = 1. dirichlet_alpha Double. Concentration parameter Dirichlet prior block assignments (used Stochastic Block model). Default: 1. lambda Double. Rate zero-truncated Poisson prior number clusters Stochastic Block Model. Default: 1. na_action Character. Specifies missing data handling. Either \"listwise\" (drop rows missing values) \"impute\" (perform single imputation sampling). Default: \"listwise\". update_method Character. Specifies MCMC sampler updates model parameters: \"adaptive-metropolis\" Componentwise adaptive Metropolis–Hastings     Robbins–Monro proposal adaptation. \"hamiltonian-mc\" Hamiltonian Monte Carlo fixed path length     (number leapfrog steps set hmc_num_leapfrogs). \"nuts\" -U-Turn Sampler, adaptive form HMC     dynamically chosen trajectory lengths. Default: \"nuts\". target_accept Numeric 0 1. Target acceptance rate sampler. Defaults set automatically supplied: 0.44 adaptive Metropolis, 0.65 HMC, 0.80 NUTS. hmc_num_leapfrogs Integer. Number leapfrog steps Hamiltonian Monte Carlo. Must positive. Default: 100. nuts_max_depth Integer. Maximum tree depth NUTS. Must positive. Default: 10. learn_mass_matrix Logical. TRUE, adapt diagonal mass matrix warmup (HMC/NUTS ). FALSE, use identity matrix. Default: FALSE. chains Integer. Number parallel chains run. Default: 4. cores Integer. Number CPU cores parallel execution. Default: parallel::detectCores(). display_progress Logical. Whether show progress bar sampling. Default: TRUE. seed Optional integer. Random seed reproducibility. Must single non-negative integer. interaction_scale, burnin, save, threshold_alpha, threshold_beta `r lifecycle::badge(\"deprecated\")` Deprecated arguments **bgms 0.1.6.0**. Use `pairwise_scale`, `warmup`, `main_alpha`, `main_beta` instead.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"list class \"bgms\" posterior summaries, posterior mean matrices, access raw MCMC draws. object can passed print(), summary(), coef(). Main components include: posterior_summary_main: Data frame posterior summaries     (mean, sd, MCSE, ESS, Rhat) category threshold parameters. posterior_summary_pairwise: Data frame posterior     summaries pairwise interaction parameters. posterior_summary_indicator: Data frame posterior     summaries edge inclusion indicators (edge_selection = TRUE). posterior_mean_main: Matrix posterior mean thresholds     (rows = variables, cols = categories parameters). posterior_mean_pairwise: Symmetric matrix posterior mean     pairwise interaction strengths. posterior_mean_indicator: Symmetric matrix posterior mean     inclusion probabilities (edge selection enabled). Additional summaries returned     edge_prior = \"Stochastic-Block\". details prior     see Sekulovski et al. (2025) . posterior_summary_pairwise_allocations: Data frame       posterior summaries (mean, sd, MCSE, ESS, Rhat) pairwise       cluster co-occurrence nodes. serves indicate       whether estimated posterior allocations,co-clustering matrix       posterior cluster probabilities (see blow) converged. posterior_coclustering_matrix: symmetric matrix       pairwise proportions occurrence every variable. matrix       can plotted visually inspect estimated number clusters       visually inspect nodes tend switch clusters. posterior_mean_allocations: vector posterior mean       cluster allocations nodes. calculated using method       proposed Dahl (2009) . posterior_mode_allocations: vector posterior        mode cluster allocations nodes. posterior_num_blocks: data frame estimated       posterior inclusion probabilities possible number clusters. raw_samples: list raw MCMC draws per chain: main List main effect samples. pairwise List pairwise effect samples. indicator List indicator samples         (edge selection enabled). allocations List cluster allocations         (SBM prior used). nchains Number chains. niter Number post–warmup iterations per chain. parameter_names Named lists parameter labels. arguments: list function call arguments metadata     (e.g., number variables, warmup, sampler settings, package version). summary() method prints formatted posterior summaries, coef() extracts posterior mean matrices. NUTS diagnostics (tree depth, divergences, energy, E-BFMI) included fit$nuts_diag update_method = \"nuts\".","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"function models joint distribution binary ordinal variables using Markov Random Field, support edge selection Bayesian variable selection. statistical foundation model described Marsman et al. (2025) , ordinal MRF model Bayesian estimation procedure first introduced. implementation bgms since extended updated (e.g., alternative priors, parallel chains, HMC/NUTS warmup), builds original framework. Key components model described sections .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"ordinal-variables","dir":"Reference","previous_headings":"","what":"Ordinal Variables","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"function supports two types ordinal variables: Regular ordinal variables: Assigns category threshold parameter response category except lowest. model imposes additional constraints distribution category responses. Blume-Capel ordinal variables: Assume baseline category (e.g., “neutral” response) score responses distance baseline. Category thresholds modeled : $$\\mu_{c} = \\alpha \\cdot c + \\beta \\cdot (c - b)^2$$ : \\(\\mu_{c}\\): category threshold category \\(c\\) \\(\\alpha\\): linear trend across categories \\(\\beta\\): preference toward away baseline \\(\\beta < 0\\), model favors responses near baseline      category; \\(\\beta > 0\\), favors responses farther away (.e.,      extremes). \\(b\\): baseline category","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"edge-selection","dir":"Reference","previous_headings":"","what":"Edge Selection","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"edge_selection = TRUE, function performs Bayesian variable selection pairwise interactions (edges) MRF using spike--slab priors. Supported priors edge inclusion: Bernoulli: Fixed inclusion probability across edges. Beta-Bernoulli: Inclusion probability assigned Beta   prior distribution. Stochastic-Block: Cluster-based edge priors Beta,   Dirichlet, Poisson hyperpriors. priors operate via binary indicator variables controlling inclusion exclusion edge MRF.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"prior-distributions","dir":"Reference","previous_headings":"","what":"Prior Distributions","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"Pairwise effects: Modeled Cauchy (slab) prior. Main effects: Modeled using beta-prime   distribution. Edge indicators: Use either Bernoulli, Beta-Bernoulli,   Stochastic-Block prior ().","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"sampling-algorithms-and-warmup","dir":"Reference","previous_headings":"","what":"Sampling Algorithms and Warmup","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"Parameters updated within Gibbs framework, conditional updates can carried using different algorithms: Adaptive Metropolis–Hastings: Componentwise random–walk     updates main effects pairwise effects. Proposal standard     deviations adapted burn–via Robbins–Monro updates     toward target acceptance rate. Hamiltonian Monte Carlo (HMC): Joint updates     parameters using fixed–length leapfrog trajectories. Step size     tuned warmup via dual–averaging; diagonal mass matrix can     also adapted learn_mass_matrix = TRUE. –U–Turn Sampler (NUTS): adaptive extension HMC     dynamically chooses trajectory lengths. Warmup uses staged     adaptation schedule (fast–slow–fast) stabilize step size ,     enabled, mass matrix. edge_selection = TRUE, updates edge–inclusion indicators carried Metropolis–Hastings steps. switched core warmup phase, ensuring graph updates occur samplers’ tuning parameters (step size, mass matrix, proposal SDs) stabilized. warmup, adaptation disabled. Step size mass matrix fixed learned values, proposal SDs remain constant.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"warmup-and-adaptation","dir":"Reference","previous_headings":"","what":"Warmup and Adaptation","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"warmup procedure bgm based multi–stage adaptation schedule used Stan (Stan Development Team 2023) . Warmup iterations split several phases: Stage 1 (fast adaptation): short initial interval     step size (HMC/NUTS) adapted, allowing chain     move quickly toward typical set. Stage 2 (slow windows): sequence expanding,     memoryless windows step size ,     learn_mass_matrix = TRUE, diagonal mass matrix     adapted. window ends reset dual–averaging scheme     improved stability. Stage 3a (final fast interval): short interval     end core warmup step size adapted one final time. Stage 3b (proposal–SD tuning): active     edge_selection = TRUE HMC/NUTS. phase,     Robbins–Monro adaptation proposal standard deviations     performed Metropolis steps used edge–selection moves. Stage 3c (graph selection warmup): Also relevant     edge_selection = TRUE. start phase,     random graph structure initialized, Metropolis–Hastings     updates edge inclusion indicators switched . edge_selection = FALSE, total number warmup iterations equals user–specified burnin. edge_selection = TRUE update_method \"nuts\" \"hamiltonian-mc\", schedule automatically appends additional Stage-3b Stage-3c intervals, total warmup strictly greater requested burnin. warmup phases, sampler transitions sampling phase adaptation disabled. Step size mass matrix (HMC/NUTS) fixed learned values, proposal SDs remain constant. staged design improves stability proposals ensures local parameters (step size) global parameters (mass matrix, proposal SDs) tuned collecting posterior samples. adaptive Metropolis–Hastings runs, step size mass matrix adaptation relevant. Proposal SDs tuned continuously burn–using Robbins–Monro updates, without staged fast/slow intervals.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"na_action = \"listwise\", observations missing values removed. na_action = \"impute\", missing values imputed Gibbs sampling.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"Dahl DB (2009). “Modal clustering class product partition models.” Bayesian Analysis, 4(2), 243–264. doi:10.1214/09-BA409 . Marsman M, van den Bergh D, Haslbeck JMB (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90, 146–-182. Sekulovski N, Arena G, Haslbeck JMB, Huth KBS, Friel N, Marsman M (2025). “Stochastic Block Prior Clustering Graphical Models.” Retrieved https://osf.io/preprints/psyarxiv/29p3m_v1. OSF preprint. Stan Development Team (2023). Stan Modeling Language Users Guide Reference Manual. Version 2.33, https://mc-stan.org/docs/.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"","code":"# \\donttest{ # Run bgm on subset of the Wenchuan dataset fit = bgm(x = Wenchuan[, 1:5]) #> Warning: There were 7 rows with missing observations in the input matrix x. #> Since na_action = listwise these rows were excluded from the analysis. #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2200 (2.3%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 38/2200 (1.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 53/2200 (2.4%) #> Chain 4 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 56/2200 (2.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 197/8800 (2.2%) #> Elapsed: 4s | ETA: 2m 54s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2200 (4.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 84/2200 (3.8%) #> Chain 3 (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 112/2200 (5.1%) #> Chain 4 (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 116/2200 (5.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 412/8800 (4.7%) #> Elapsed: 8s | ETA: 2m 42s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 150/2200 (6.8%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 142/2200 (6.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 154/2200 (7.0%) #> Chain 4 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 173/2200 (7.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 619/8800 (7.0%) #> Elapsed: 12s | ETA: 2m 38s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2200 (9.1%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 197/2200 (9.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 201/2200 (9.1%) #> Chain 4 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 224/2200 (10.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 822/8800 (9.3%) #> Elapsed: 15s | ETA: 2m 25s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2200 (11.4%) #> Chain 2 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 246/2200 (11.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 249/2200 (11.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 268/2200 (12.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1013/8800 (11.5%) #> Elapsed: 19s | ETA: 2m 26s #> Chain 1 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2200 (13.6%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 307/2200 (14.0%) #> Chain 3 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 296/2200 (13.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 325/2200 (14.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1228/8800 (14.0%) #> Elapsed: 22s | ETA: 2m 15s #> Chain 1 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 350/2200 (15.9%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 368/2200 (16.7%) #> Chain 3 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 349/2200 (15.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 381/2200 (17.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1448/8800 (16.5%) #> Elapsed: 26s | ETA: 2m 12s #> Chain 1 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2200 (18.2%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 419/2200 (19.0%) #> Chain 3 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2200 (18.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 438/2200 (19.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1657/8800 (18.8%) #> Elapsed: 30s | ETA: 2m 9s #> Chain 1 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 450/2200 (20.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 485/2200 (22.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 449/2200 (20.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 507/2200 (23.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1891/8800 (21.5%) #> Elapsed: 33s | ETA: 2m #> Chain 1 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2200 (22.7%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 524/2200 (23.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 490/2200 (22.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 543/2200 (24.7%) #> Total   (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2057/8800 (23.4%) #> Elapsed: 36s | ETA: 1m 58s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 550/2200 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 582/2200 (26.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 541/2200 (24.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 597/2200 (27.1%) #> Total   (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2270/8800 (25.8%) #> Elapsed: 39s | ETA: 1m 52s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 600/2200 (27.3%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 625/2200 (28.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 582/2200 (26.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 648/2200 (29.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2455/8800 (27.9%) #> Elapsed: 42s | ETA: 1m 48s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2200 (29.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 696/2200 (31.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 648/2200 (29.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 711/2200 (32.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2705/8800 (30.7%) #> Elapsed: 46s | ETA: 1m 43s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 700/2200 (31.8%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 745/2200 (33.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 699/2200 (31.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 762/2200 (34.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2906/8800 (33.0%) #> Elapsed: 49s | ETA: 1m 39s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 750/2200 (34.1%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 780/2200 (35.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 743/2200 (33.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 804/2200 (36.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3077/8800 (35.0%) #> Elapsed: 52s | ETA: 1m 36s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2200 (36.4%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 818/2200 (37.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 789/2200 (35.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 845/2200 (38.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3252/8800 (37.0%) #> Elapsed: 55s | ETA: 1m 33s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 850/2200 (38.6%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 860/2200 (39.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 841/2200 (38.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 891/2200 (40.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3442/8800 (39.1%) #> Elapsed: 58s | ETA: 1m 30s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2200 (40.9%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 907/2200 (41.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 884/2200 (40.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 940/2200 (42.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3631/8800 (41.3%) #> Elapsed: 1m 1s | ETA: 1m 26s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 950/2200 (43.2%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 968/2200 (44.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 935/2200 (42.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 991/2200 (45.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 3844/8800 (43.7%) #> Elapsed: 1m 5s | ETA: 1m 23s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 1000/2200 (45.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 1013/2200 (46.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 982/2200 (44.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1037/2200 (47.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 4032/8800 (45.8%) #> Elapsed: 1m 8s | ETA: 1m 20s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 1050/2200 (47.7%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 1062/2200 (48.3%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1025/2200 (46.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1085/2200 (49.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 4222/8800 (48.0%) #> Elapsed: 1m 11s | ETA: 1m 16s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2200 (50.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1115/2200 (50.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1080/2200 (49.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1164/2200 (52.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 4459/8800 (50.7%) #> Elapsed: 1m 15s | ETA: 1m 13s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1150/2200 (52.3%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1146/2200 (52.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1116/2200 (50.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1194/2200 (54.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4606/8800 (52.3%) #> Elapsed: 1m 17s | ETA: 1m 10s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2200 (54.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1185/2200 (53.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1160/2200 (52.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1241/2200 (56.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4786/8800 (54.4%) #> Elapsed: 1m 19s | ETA: 1m 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2200 (56.8%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1214/2200 (55.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1207/2200 (54.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1278/2200 (58.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 4949/8800 (56.2%) #> Elapsed: 1m 21s | ETA: 1m 3s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1300/2200 (59.1%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1262/2200 (57.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1283/2200 (58.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1336/2200 (60.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5181/8800 (58.9%) #> Elapsed: 1m 24s | ETA: 59s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1350/2200 (61.4%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1306/2200 (59.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1342/2200 (61.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1389/2200 (63.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 5387/8800 (61.2%) #> Elapsed: 1m 27s | ETA: 55s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1400/2200 (63.6%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1342/2200 (61.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1387/2200 (63.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1434/2200 (65.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 5563/8800 (63.2%) #> Elapsed: 1m 29s | ETA: 52s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1450/2200 (65.9%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1390/2200 (63.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1447/2200 (65.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1487/2200 (67.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 5774/8800 (65.6%) #> Elapsed: 1m 32s | ETA: 48s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1500/2200 (68.2%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1428/2200 (64.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1493/2200 (67.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1528/2200 (69.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 5949/8800 (67.6%) #> Elapsed: 1m 35s | ETA: 46s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1550/2200 (70.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1477/2200 (67.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1544/2200 (70.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1582/2200 (71.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6153/8800 (69.9%) #> Elapsed: 1m 38s | ETA: 42s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1600/2200 (72.7%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1517/2200 (69.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1596/2200 (72.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1638/2200 (74.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6351/8800 (72.2%) #> Elapsed: 1m 41s | ETA: 39s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2200 (75.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1553/2200 (70.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1640/2200 (74.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1682/2200 (76.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6525/8800 (74.1%) #> Elapsed: 1m 43s | ETA: 36s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1700/2200 (77.3%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1589/2200 (72.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1688/2200 (76.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1719/2200 (78.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 6696/8800 (76.1%) #> Elapsed: 1m 46s | ETA: 33s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2200 (79.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1638/2200 (74.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1742/2200 (79.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1761/2200 (80.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 6891/8800 (78.3%) #> Elapsed: 1m 48s | ETA: 30s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2200 (81.8%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1679/2200 (76.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1790/2200 (81.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1812/2200 (82.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 7081/8800 (80.5%) #> Elapsed: 1m 51s | ETA: 27s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1850/2200 (84.1%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1713/2200 (77.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1837/2200 (83.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1856/2200 (84.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7256/8800 (82.5%) #> Elapsed: 1m 53s | ETA: 24s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1900/2200 (86.4%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1756/2200 (79.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1890/2200 (85.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1898/2200 (86.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7444/8800 (84.6%) #> Elapsed: 1m 56s | ETA: 21s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1950/2200 (88.6%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2200 (81.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1937/2200 (88.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1949/2200 (88.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7636/8800 (86.8%) #> Elapsed: 1m 59s | ETA: 18s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 2000/2200 (90.9%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1848/2200 (84.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1990/2200 (90.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 2004/2200 (91.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7842/8800 (89.1%) #> Elapsed: 2m 2s | ETA: 15s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 2050/2200 (93.2%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1898/2200 (86.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 2040/2200 (92.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 2053/2200 (93.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8041/8800 (91.4%) #> Elapsed: 2m 4s | ETA: 12s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 2100/2200 (95.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1946/2200 (88.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 2106/2200 (95.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 2113/2200 (96.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8265/8800 (93.9%) #> Elapsed: 2m 8s | ETA: 8s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 2150/2200 (97.7%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1997/2200 (90.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 2151/2200 (97.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 2165/2200 (98.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 8463/8800 (96.2%) #> Elapsed: 2m 10s | ETA: 5s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 2052/2200 (93.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 8652/8800 (98.3%) #> Elapsed: 2m 13s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8800/8800 (100.0%) #> Elapsed: 2m 17s | ETA: 0s #> NUTS Diagnostics Summary: #>   Total divergences:         0  #>   Max tree depth hits:       0  #>   Min E-BFMI across chains:  1.41   # Posterior inclusion probabilities summary(fit)$indicator #>                     mean         sd        mcse n0->0 n0->1 n1->0 #> intrusion-dreams  1.0000 0.00000000          NA     0     0     0 #> intrusion-flash   1.0000 0.00000000          NA     0     0     0 #> intrusion-upset   0.9665 0.17993818 0.020278792   129     5     5 #> intrusion-physior 0.9705 0.16920328 0.010105943   103    15    15 #> dreams-flash      1.0000 0.00000000          NA     0     0     0 #> dreams-upset      0.9980 0.04467662 0.002732952     7     1     1 #> dreams-physior    0.0655 0.24740604 0.010369949  3676    61    61 #> flash-upset       0.0780 0.26817159 0.010326797  3604    83    83 #> flash-physior     1.0000 0.00000000          NA     0     0     0 #> upset-physior     1.0000 0.00000000          NA     0     0     0 #>                   n1->1     n_eff     Rhat #> intrusion-dreams   3999        NA       NA #> intrusion-flash    3999        NA       NA #> intrusion-upset    3860  78.73403 1.158955 #> intrusion-physior  3866 280.32633 1.036134 #> dreams-flash       3999        NA       NA #> dreams-upset       3990 267.23691 1.295115 #> dreams-physior      201 569.20319 1.023299 #> flash-upset         229 674.36385 1.010784 #> flash-physior      3999        NA       NA #> upset-physior      3999        NA       NA  # Posterior pairwise effects summary(fit)$pairwise #>                          mean          sd         mcse     n_eff #> intrusion-dreams  0.628898981 0.001354364 0.0637885338 2218.2706 #> intrusion-flash   0.339822281 0.001232575 0.0607533885 2429.4852 #> intrusion-upset   0.197986390 0.066096936 0.0043653239  229.2607 #> intrusion-physior 0.199514759 0.063735200 0.0023380170  743.1276 #> dreams-flash      0.501004911 0.001186768 0.0593027544 2496.9956 #> dreams-upset      0.227193963 0.054624196 0.0016087760 1152.8663 #> dreams-physior    0.006439865 0.024642053 0.0010550413  545.5252 #> flash-upset       0.007308683 0.025481703 0.0009968409  653.4393 #> flash-physior     0.307080088 0.001002044 0.0526542460 2761.1683 #> upset-physior     0.710712274 0.001336213 0.0594773874 1981.3097 #>                        Rhat #> intrusion-dreams  1.0007127 #> intrusion-flash   1.0003860 #> intrusion-upset   1.0098734 #> intrusion-physior 1.0034069 #> dreams-flash      0.9998348 #> dreams-upset      1.0012725 #> dreams-physior    1.0049068 #> flash-upset       1.0026063 #> flash-physior     1.0001797 #> upset-physior     1.0004924 # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"bgmCompare function estimates group differences category threshold parameters (main effects) pairwise interactions (pairwise effects) Markov Random Field (MRF) binary ordinal variables. Groups can defined either supplying two separate datasets (x y) group membership vector. Optionally, Bayesian variable selection can applied identify differences across groups.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"","code":"bgmCompare(   x,   y,   group_indicator,   difference_selection = TRUE,   variable_type = \"ordinal\",   baseline_category,   difference_scale = 1,   difference_prior = c(\"Bernoulli\", \"Beta-Bernoulli\"),   difference_probability = 0.5,   beta_bernoulli_alpha = 1,   beta_bernoulli_beta = 1,   pairwise_scale = 2.5,   main_alpha = 0.5,   main_beta = 0.5,   iter = 1000,   warmup = 1000,   na_action = c(\"listwise\", \"impute\"),   update_method = c(\"nuts\", \"adaptive-metropolis\", \"hamiltonian-mc\"),   target_accept,   hmc_num_leapfrogs = 100,   nuts_max_depth = 10,   learn_mass_matrix = FALSE,   chains = 4,   cores = parallel::detectCores(),   display_progress = c(\"per-chain\", \"total\", \"none\"),   seed = NULL,   main_difference_model,   reference_category,   main_difference_scale,   pairwise_difference_scale,   pairwise_difference_prior,   main_difference_prior,   pairwise_difference_probability,   main_difference_probability,   pairwise_beta_bernoulli_alpha,   pairwise_beta_bernoulli_beta,   main_beta_bernoulli_alpha,   main_beta_bernoulli_beta,   interaction_scale,   threshold_alpha,   threshold_beta,   burnin,   save )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"x data frame matrix binary ordinal responses Group 1. Variables coded nonnegative integers starting 0. ordinal variables, unused categories collapsed; Blume–Capel variables, categories retained. y Optional data frame matrix Group 2 (two-group designs). Must variables (columns) x. group_indicator Optional integer vector group memberships rows x (multi-group designs). Ignored y supplied. difference_selection Logical. TRUE, spike--slab priors applied difference parameters. Default: TRUE. variable_type Character vector specifying type variable: \"ordinal\" (default) \"blume-capel\". baseline_category Integer vector giving baseline category Blume–Capel variables. difference_scale Double. Scale Cauchy prior difference parameters. Default: 1. difference_prior Character. Prior difference inclusion: \"Bernoulli\" \"Beta-Bernoulli\". Default: \"Bernoulli\". difference_probability Numeric. Prior inclusion probability differences (Bernoulli prior). Default: 0.5. beta_bernoulli_alpha, beta_bernoulli_beta Doubles. Shape parameters Beta prior inclusion probabilities Beta–Bernoulli model. Defaults: 1. pairwise_scale Double. Scale Cauchy prior baseline pairwise interactions. Default: 2.5. main_alpha, main_beta Doubles. Shape parameters beta-prime prior baseline threshold parameters. Defaults: 0.5. iter Integer. Number post–warmup iterations per chain. Default: 1e3. warmup Integer. Number warmup iterations sampling. Default: 1e3. na_action Character. handle missing data: \"listwise\" (drop rows) \"impute\" (impute within Gibbs). Default: \"listwise\". update_method Character. Sampling algorithm: \"adaptive-metropolis\", \"hamiltonian-mc\", \"nuts\". Default: \"nuts\". target_accept Numeric 0 1. Target acceptance rate. Defaults: 0.44 (Metropolis), 0.65 (HMC), 0.80 (NUTS). hmc_num_leapfrogs Integer. Leapfrog steps HMC. Default: 100. nuts_max_depth Integer. Maximum tree depth NUTS. Default: 10. learn_mass_matrix Logical. TRUE, adapt mass matrix warmup (HMC/NUTS ). Default: FALSE. chains Integer. Number parallel chains. Default: 4. cores Integer. Number CPU cores. Default: parallel::detectCores(). display_progress Character. Controls progress reporting: \"per-chain\", \"total\", \"none\". Default: \"per-chain\". seed Optional integer. Random seed reproducibility. main_difference_model, reference_category, pairwise_difference_scale, main_difference_scale, pairwise_difference_prior, main_difference_prior, pairwise_difference_probability, main_difference_probability, pairwise_beta_bernoulli_alpha, pairwise_beta_bernoulli_beta, main_beta_bernoulli_alpha, main_beta_bernoulli_beta, interaction_scale, threshold_alpha, threshold_beta, burnin, save `r lifecycle::badge(\"deprecated\")` Deprecated arguments **bgms 0.1.6.0**. Use `difference_scale`, `difference_prior`, `difference_probability`, `beta_bernoulli_alpha`, `beta_bernoulli_beta`, `baseline_category`, `pairwise_scale`, `warmup` instead.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"list class \"bgmCompare\" containing posterior summaries, posterior mean matrices, raw MCMC samples: posterior_summary_main_baseline,     posterior_summary_pairwise_baseline: summaries baseline     thresholds pairwise interactions. posterior_summary_main_differences,     posterior_summary_pairwise_differences: summaries group     differences thresholds pairwise interactions. posterior_summary_indicator: summaries inclusion     indicators (difference_selection = TRUE). posterior_mean_main_baseline,     posterior_mean_pairwise_baseline: posterior mean matrices     (legacy style). raw_samples: list raw draws per chain main,     pairwise, indicator parameters. arguments: list function call arguments metadata. summary() method prints formatted summaries, coef() extracts posterior means. NUTS diagnostics (tree depth, divergences, energy, E-BFMI) included fit$nuts_diag update_method = \"nuts\".","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"function extends ordinal MRF framework Marsman et al. (2025)  multiple groups. basic idea modeling, analyzing, testing group differences MRFs introduced Marsman et al. (2024) , two–group comparisons conducted using adaptive Metropolis sampling. present implementation generalizes approach two groups supports additional samplers (HMC NUTS) staged warmup adaptation. Key components model:","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"pairwise-interactions","dir":"Reference","previous_headings":"","what":"Pairwise Interactions","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"variables \\(\\) \\(j\\), group-specific interaction represented : $$\\theta_{ij}^{(g)} = \\phi_{ij} + \\delta_{ij}^{(g)},$$ \\(\\phi_{ij}\\) baseline effect \\(\\delta_{ij}^{(g)}\\) group differences constrained sum zero.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"ordinal-variables","dir":"Reference","previous_headings":"","what":"Ordinal Variables","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"Regular ordinal variables: category thresholds decomposed baseline plus group differences category. Blume–Capel variables: category thresholds quadratic category index, linear quadratic terms split baseline plus group differences.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"variable-selection","dir":"Reference","previous_headings":"","what":"Variable Selection","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"difference_selection = TRUE, spike--slab priors applied difference parameters: Bernoulli: fixed prior inclusion probability. Beta–Bernoulli: inclusion probability given Beta prior.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"sampling-algorithms-and-warmup","dir":"Reference","previous_headings":"","what":"Sampling Algorithms and Warmup","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"Parameters updated within Gibbs framework, using sampling algorithms staged warmup scheme described bgm: Adaptive Metropolis–Hastings: componentwise random–walk     proposals Robbins–Monro adaptation proposal SDs. Hamiltonian Monte Carlo (HMC): joint updates fixed     leapfrog trajectories; step size optionally mass matrix     adapted warmup. –U–Turn Sampler (NUTS): adaptive HMC variant     dynamic trajectory lengths; warmup uses staged adaptation     schedule HMC. details staged adaptation schedule (fast–slow–fast phases), see bgm. addition, difference_selection = TRUE, updates inclusion indicators delayed late warmup. HMC/NUTS, appends two extra phases (Stage-3b Stage-3c), total number warmup iterations exceeds user-specified warmup. warmup, adaptation disabled: step size mass matrix fixed learned values, proposal SDs remain constant.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"Marsman M, van den Bergh D, Haslbeck JMB (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90, 146–-182. Marsman M, Waldorp LJ, Sekulovski N, Haslbeck JMB (2024). “Bayes factor tests group differences ordinal binary graphical models.” Retrieved https://osf.io/preprints/osf/f4pk9. OSF preprint.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"","code":"# \\dontrun{ # Run bgmCompare on subset of the Boredom dataset x = Boredom[Boredom$language == \"fr\", 2:6] y = Boredom[Boredom$language != \"fr\", 2:6]  fit <- bgmCompare(x, y) #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2200 (2.3%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 46/2200 (2.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 44/2200 (2.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 52/2200 (2.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 192/8800 (2.2%) #> Elapsed: 35s | ETA: 26m 9s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2200 (4.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 91/2200 (4.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 98/2200 (4.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 107/2200 (4.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 396/8800 (4.5%) #> Elapsed: 1m 20s | ETA: 28m 17s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 150/2200 (6.8%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 147/2200 (6.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 149/2200 (6.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 155/2200 (7.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 601/8800 (6.8%) #> Elapsed: 2m 1s | ETA: 27m 30s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2200 (9.1%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 197/2200 (9.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 197/2200 (9.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 210/2200 (9.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 804/8800 (9.1%) #> Elapsed: 2m 42s | ETA: 26m 51s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2200 (11.4%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2200 (11.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 249/2200 (11.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 259/2200 (11.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1008/8800 (11.5%) #> Elapsed: 3m 22s | ETA: 26m 1s #> Chain 1 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2200 (13.6%) #> Chain 2 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 292/2200 (13.3%) #> Chain 3 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 292/2200 (13.3%) #> Chain 4 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 301/2200 (13.7%) #> Total   (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1185/8800 (13.5%) #> Elapsed: 3m 54s | ETA: 25m 3s #> Chain 1 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 350/2200 (15.9%) #> Chain 2 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 345/2200 (15.7%) #> Chain 3 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 348/2200 (15.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 359/2200 (16.3%) #> Total   (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1402/8800 (15.9%) #> Elapsed: 4m 36s | ETA: 24m 16s #> Chain 1 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2200 (18.2%) #> Chain 2 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 402/2200 (18.3%) #> Chain 3 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 398/2200 (18.1%) #> Chain 4 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 411/2200 (18.7%) #> Total   (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1611/8800 (18.3%) #> Elapsed: 5m 14s | ETA: 23m 21s #> Chain 1 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 450/2200 (20.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 452/2200 (20.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 447/2200 (20.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 464/2200 (21.1%) #> Total   (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1813/8800 (20.6%) #> Elapsed: 5m 50s | ETA: 22m 28s #> Chain 1 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2200 (22.7%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 494/2200 (22.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 489/2200 (22.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 512/2200 (23.3%) #> Total   (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1995/8800 (22.7%) #> Elapsed: 6m 26s | ETA: 21m 56s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 550/2200 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 539/2200 (24.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 526/2200 (23.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 560/2200 (25.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2175/8800 (24.7%) #> Elapsed: 7m | ETA: 21m 19s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 600/2200 (27.3%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 589/2200 (26.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 588/2200 (26.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 617/2200 (28.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2394/8800 (27.2%) #> Elapsed: 7m 39s | ETA: 20m 28s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2200 (29.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 637/2200 (29.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 636/2200 (28.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 660/2200 (30.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2583/8800 (29.4%) #> Elapsed: 8m 11s | ETA: 19m 41s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 700/2200 (31.8%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 679/2200 (30.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 688/2200 (31.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 712/2200 (32.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2779/8800 (31.6%) #> Elapsed: 8m 47s | ETA: 19m 1s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 750/2200 (34.1%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 730/2200 (33.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 734/2200 (33.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 755/2200 (34.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2969/8800 (33.7%) #> Elapsed: 9m 20s | ETA: 18m 19s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2200 (36.4%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 781/2200 (35.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 787/2200 (35.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 808/2200 (36.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3176/8800 (36.1%) #> Elapsed: 9m 57s | ETA: 17m 37s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 850/2200 (38.6%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 830/2200 (37.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 838/2200 (38.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 854/2200 (38.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3372/8800 (38.3%) #> Elapsed: 10m 35s | ETA: 17m 2s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2200 (40.9%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 863/2200 (39.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 887/2200 (40.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 897/2200 (40.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 3547/8800 (40.3%) #> Elapsed: 11m 8s | ETA: 16m 29s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 950/2200 (43.2%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 920/2200 (41.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 936/2200 (42.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 953/2200 (43.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 3759/8800 (42.7%) #> Elapsed: 11m 45s | ETA: 15m 45s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 1000/2200 (45.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 973/2200 (44.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 991/2200 (45.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 1002/2200 (45.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 3966/8800 (45.1%) #> Elapsed: 12m 22s | ETA: 15m 4s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 1050/2200 (47.7%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1022/2200 (46.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 1050/2200 (47.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 1051/2200 (47.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4173/8800 (47.4%) #> Elapsed: 12m 58s | ETA: 14m 22s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2200 (50.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 1067/2200 (48.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1097/2200 (49.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1095/2200 (49.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4359/8800 (49.5%) #> Elapsed: 13m 32s | ETA: 13m 47s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1150/2200 (52.3%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1097/2200 (49.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1153/2200 (52.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1146/2200 (52.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4546/8800 (51.7%) #> Elapsed: 13m 52s | ETA: 12m 58s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2200 (54.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1137/2200 (51.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1205/2200 (54.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2200 (54.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4742/8800 (53.9%) #> Elapsed: 14m 10s | ETA: 12m 7s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2200 (56.8%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1167/2200 (53.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1240/2200 (56.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1244/2200 (56.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 4901/8800 (55.7%) #> Elapsed: 14m 24s | ETA: 11m 27s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1300/2200 (59.1%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1220/2200 (55.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1294/2200 (58.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1295/2200 (58.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 5109/8800 (58.1%) #> Elapsed: 14m 43s | ETA: 10m 37s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1350/2200 (61.4%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1264/2200 (57.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1336/2200 (60.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1342/2200 (61.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 5292/8800 (60.1%) #> Elapsed: 15m 1s | ETA: 9m 57s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1400/2200 (63.6%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1319/2200 (60.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1383/2200 (62.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1389/2200 (63.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5491/8800 (62.4%) #> Elapsed: 15m 19s | ETA: 9m 13s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1450/2200 (65.9%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1391/2200 (63.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1440/2200 (65.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1456/2200 (66.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 5737/8800 (65.2%) #> Elapsed: 15m 42s | ETA: 8m 22s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1500/2200 (68.2%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1444/2200 (65.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1485/2200 (67.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1495/2200 (68.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5924/8800 (67.3%) #> Elapsed: 16m | ETA: 7m 46s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1550/2200 (70.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1484/2200 (67.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1541/2200 (70.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1539/2200 (70.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6114/8800 (69.5%) #> Elapsed: 16m 18s | ETA: 7m 9s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1600/2200 (72.7%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1528/2200 (69.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1589/2200 (72.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1578/2200 (71.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6295/8800 (71.5%) #> Elapsed: 16m 37s | ETA: 6m 36s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2200 (75.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1576/2200 (71.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1630/2200 (74.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1627/2200 (74.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 6483/8800 (73.7%) #> Elapsed: 16m 55s | ETA: 6m 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1700/2200 (77.3%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1622/2200 (73.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1671/2200 (76.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1672/2200 (76.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 6665/8800 (75.7%) #> Elapsed: 17m 12s | ETA: 5m 30s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2200 (79.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1678/2200 (76.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1724/2200 (78.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1723/2200 (78.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 6875/8800 (78.1%) #> Elapsed: 17m 31s | ETA: 4m 54s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2200 (81.8%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1736/2200 (78.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1777/2200 (80.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1770/2200 (80.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 7083/8800 (80.5%) #> Elapsed: 17m 51s | ETA: 4m 19s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1850/2200 (84.1%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1785/2200 (81.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1831/2200 (83.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1818/2200 (82.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 7284/8800 (82.8%) #> Elapsed: 18m 9s | ETA: 3m 46s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1900/2200 (86.4%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1843/2200 (83.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1876/2200 (85.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1869/2200 (85.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 7488/8800 (85.1%) #> Elapsed: 18m 30s | ETA: 3m 14s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1950/2200 (88.6%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1905/2200 (86.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1925/2200 (87.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1925/2200 (87.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 7705/8800 (87.6%) #> Elapsed: 18m 52s | ETA: 2m 40s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 2000/2200 (90.9%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1962/2200 (89.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1973/2200 (89.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1978/2200 (89.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7913/8800 (89.9%) #> Elapsed: 19m 12s | ETA: 2m 9s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 2050/2200 (93.2%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2010/2200 (91.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2027/2200 (92.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2027/2200 (92.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8114/8800 (92.2%) #> Elapsed: 19m 31s | ETA: 1m 39s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 2100/2200 (95.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2071/2200 (94.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2080/2200 (94.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2087/2200 (94.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8338/8800 (94.8%) #> Elapsed: 19m 52s | ETA: 1m 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 2150/2200 (97.7%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 2117/2200 (96.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2125/2200 (96.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2121/2200 (96.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8513/8800 (96.7%) #> Elapsed: 20m 9s | ETA: 41s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 2167/2200 (98.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2174/2200 (98.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 2165/2200 (98.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8706/8800 (98.9%) #> Elapsed: 20m 27s | ETA: 13s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2200/2200 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8800/8800 (100.0%) #> Elapsed: 20m 40s | ETA: 0s #> NUTS Diagnostics Summary: #>   Total divergences:         0  #>   Max tree depth hits:       16  #>   Min E-BFMI across chains:  1.347  #> Note: 0.40% of transitions hit the maximum tree depth (16 of 4000). #> Check efficiency metrics such as effective sample size (ESS) to ensure #> sufficient exploration of the posterior.  # Posterior inclusion probabilities summary(fit)$indicator #>                            parameter    mean         sd         mcse #> 1                  loose_ends (main) 0.00800 0.08908423 0.0021553353 #> 2    loose_ends-entertain (pairwise) 0.02325 0.15069651 0.0027163816 #> 3   loose_ends-repetitive (pairwise) 0.49075 0.49991443 0.0178398238 #> 4  loose_ends-stimulation (pairwise) 0.03250 0.17732386 0.0030066487 #> 5    loose_ends-motivated (pairwise) 0.01825 0.13385417 0.0021967141 #> 6                   entertain (main) 0.00125 0.03533324 0.0005579686 #> 7    entertain-repetitive (pairwise) 0.17450 0.37953886 0.0127866893 #> 8   entertain-stimulation (pairwise) 0.07300 0.26013650 0.0047317358 #> 9     entertain-motivated (pairwise) 0.10750 0.30974788 0.0078007749 #> 10                 repetitive (main) 0.02350 0.15148515 0.0086762804 #> 11 repetitive-stimulation (pairwise) 0.03000 0.17058722 0.0030278351 #> 12   repetitive-motivated (pairwise) 0.35100 0.47728293 0.0178234069 #> 13                stimulation (main) 0.00825 0.09045406 0.0031821604 #> 14  stimulation-motivated (pairwise) 0.01450 0.11953974 0.0019288038 #> 15                  motivated (main) 0.00525 0.07226643 0.0013133220 #>    n0->0 n0->1 n1->0 n1->1     n_eff     Rhat #> 1   3948    19    19    13 1708.3308 1.008716 #> 2   3827    79    79    14 3077.6897 1.010161 #> 3   1709   328   328  1634  785.2541 1.004997 #> 4   3752   117   117    13 3478.3155 1.044929 #> 5   3857    69    69     4 3712.9295 1.001704 #> 6   3989     5     5     0 4010.0276 1.018991 #> 7   3093   208   208   490  881.0413 1.005220 #> 8   3474   233   233    59 3022.4674 1.000508 #> 9   3352   217   217   213 1576.6714 1.007713 #> 10  3892    13    13    81  304.8407 1.171066 #> 11  3776   103   103    17 3174.1581 1.016748 #> 12  2318   277   277  1127  717.0846 1.012312 #> 13  3955    11    11    22  808.0012 1.098707 #> 14  3885    56    56     2 3841.0373 1.005052 #> 15  3960    18    18     3 3027.8251 1.012673  # Bayesian model averaged main effects for the groups coef(fit)$main_effects_groups #>                      group1      group2 #> loose_ends(c1)   -0.9389783  -0.9388797 #> loose_ends(c2)   -2.5237622  -2.5201203 #> loose_ends(c3)   -3.8076627  -3.8051071 #> loose_ends(c4)   -5.0947767  -5.0917300 #> loose_ends(c5)   -7.6196143  -7.6191916 #> loose_ends(c6)  -10.1230928 -10.1264745 #> entertain(c1)    -0.8676932  -0.8678731 #> entertain(c2)    -2.2405777  -2.2407409 #> entertain(c3)    -3.8072241  -3.8064777 #> entertain(c4)    -5.1598361  -5.1597199 #> entertain(c5)    -7.0271221  -7.0271033 #> entertain(c6)    -9.5551446  -9.5547415 #> repetitive(c1)   -0.1328744  -0.1383935 #> repetitive(c2)   -0.6519443  -0.6617877 #> repetitive(c3)   -1.0830117  -1.0862527 #> repetitive(c4)   -1.8605245  -1.8550559 #> repetitive(c5)   -3.2519322  -3.2390027 #> repetitive(c6)   -5.0336443  -5.0203641 #> stimulation(c1)  -0.5449405  -0.5496187 #> stimulation(c2)  -1.7869288  -1.7882104 #> stimulation(c3)  -2.5323528  -2.5355466 #> stimulation(c4)  -3.6175162  -3.6229256 #> stimulation(c5)  -5.1020441  -5.1059026 #> stimulation(c6)  -7.0722783  -7.0802112 #> motivated(c1)    -0.5548375  -0.5560712 #> motivated(c2)    -1.8060848  -1.8073630 #> motivated(c3)    -3.2965281  -3.2958564 #> motivated(c4)    -4.7930806  -4.7917099 #> motivated(c5)    -6.7887642  -6.7906473 #> motivated(c6)    -9.1574770  -9.1569302  # Bayesian model averaged pairwise effects for the groups coef(fit)$pairwise_effects_groups #>                            group1     group2 #> loose_ends-entertain   0.16944801 0.16965255 #> loose_ends-repetitive  0.04796648 0.06866642 #> loose_ends-stimulation 0.12653282 0.12724064 #> loose_ends-motivated   0.14147065 0.14131006 #> entertain-repetitive   0.06326749 0.06921378 #> entertain-stimulation  0.10820798 0.11029493 #> entertain-motivated    0.08431375 0.08786600 #> repetitive-stimulation 0.05603818 0.05673127 #> repetitive-motivated   0.13086773 0.14423372 #> stimulation-motivated  0.10795382 0.10800841 # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"R package bgms provides tools Bayesian analysis ordinal Markov random field (MRF), graphical model describing networks binary /ordinal variables (Marsman et al. 2025) . likelihood approximated via pseudolikelihood, Markov chain Monte Carlo (MCMC) methods used sample corresponding pseudoposterior distribution model parameters. main entry points : bgm: estimation one-sample design. bgmCompare: estimation group comparison         independent-sample design. functions support Bayesian effect selection spike--slab priors. one-sample designs, bgm models presence absence   edges variables. Posterior inclusion probabilities quantify   plausibility edge can converted Bayes factors   conditional independence tests. bgm can also model communities (clusters) variables.   posterior distribution number clusters provides evidence   clustering (Sekulovski et al. 2025) . independent-sample designs, bgmCompare estimates group   differences edge weights category thresholds. Posterior inclusion   probabilities quantify evidence differences can converted   Bayes factors parameter equivalence tests   (Marsman et al. 2024) .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"tools","dir":"Reference","previous_headings":"","what":"Tools","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"package also provides: Simulation response data MRFs Gibbs sampler         (mrfSampler). Posterior estimation edge selection one-sample designs         (bgm). Posterior estimation group-difference selection         independent-sample designs (bgmCompare).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"vignettes","dir":"Reference","previous_headings":"","what":"Vignettes","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"tutorials worked examples, see: vignette(\"intro\", package = \"bgms\") — Getting started. vignette(\"comparison\", package = \"bgms\") — Model comparison. vignette(\"diagnostics\", package = \"bgms\") — Diagnostics         spike--slab summaries.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"Marsman M, van den Bergh D, Haslbeck JMB (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90, 146–-182. Marsman M, Waldorp LJ, Sekulovski N, Haslbeck JMB (2024). “Bayes factor tests group differences ordinal binary graphical models.” Retrieved https://osf.io/preprints/osf/f4pk9. OSF preprint. Sekulovski N, Arena G, Haslbeck JMB, Huth KBS, Friel N, Marsman M (2025). “Stochastic Block Prior Clustering Graphical Models.” Retrieved https://osf.io/preprints/psyarxiv/29p3m_v1. OSF preprint.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"Maintainer: Maarten Marsman m.marsman@uva.nl (ORCID) contributors: Giuseppe Arena (ORCID) [contributor] Karoline Huth (ORCID) [contributor] Nikola Sekulovski (ORCID) [contributor] Don van den Bergh (ORCID) [contributor]","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"Returns posterior means raw parameters (baseline + differences) group-specific effects bgmCompare fit, well inclusion indicators.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' coef(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"object object class bgmCompare. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"list components: main_effects_raw Posterior means raw main-effect parameters   (variables x [baseline + differences]). pairwise_effects_raw Posterior means raw pairwise-effect parameters   (pairs x [baseline + differences]). main_effects_groups Posterior means group-specific main effects   (variables x groups), computed baseline plus projected differences. pairwise_effects_groups Posterior means group-specific pairwise effects   (pairs x groups), computed baseline plus projected differences. indicators Posterior mean inclusion probabilities symmetric matrix,   diagonals corresponding main effects -diagonals pairwise effects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficients from a bgms Object — coef.bgms","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"Returns posterior mean thresholds, pairwise effects, edge inclusion indicators bgms model fit.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"","code":"# S3 method for class 'bgms' coef(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"object object class bgms. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"list following components: main Posterior mean category threshold parameters. pairwise Posterior mean pairwise interaction matrix. indicator Posterior mean edge inclusion indicators (available).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Extractor Functions for bgms Objects — extractor_functions","title":"Extractor Functions for bgms Objects — extractor_functions","text":"Extractor Functions bgms Objects","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extractor Functions for bgms Objects — extractor_functions","text":"","code":"extract_arguments(bgms_object)  # S3 method for class 'bgms' extract_arguments(bgms_object)  # S3 method for class 'bgmCompare' extract_arguments(bgms_object)  extract_indicators(bgms_object)  # S3 method for class 'bgms' extract_indicators(bgms_object)  # S3 method for class 'bgmCompare' extract_indicators(bgms_object)  extract_posterior_inclusion_probabilities(bgms_object)  # S3 method for class 'bgms' extract_posterior_inclusion_probabilities(bgms_object)  extract_sbm(bgms_object)  # S3 method for class 'bgms' extract_sbm(bgms_object)  # S3 method for class 'bgmCompare' extract_posterior_inclusion_probabilities(bgms_object)  extract_indicator_priors(bgms_object)  # S3 method for class 'bgms' extract_indicator_priors(bgms_object)  # S3 method for class 'bgmCompare' extract_indicator_priors(bgms_object)  extract_pairwise_interactions(bgms_object)  # S3 method for class 'bgms' extract_pairwise_interactions(bgms_object)  # S3 method for class 'bgmCompare' extract_pairwise_interactions(bgms_object)  extract_category_thresholds(bgms_object)  # S3 method for class 'bgms' extract_category_thresholds(bgms_object)  # S3 method for class 'bgmCompare' extract_category_thresholds(bgms_object)  extract_group_params(bgms_object)  # S3 method for class 'bgmCompare' extract_group_params(bgms_object)  extract_edge_indicators(bgms_object)  extract_pairwise_thresholds(bgms_object)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extractor Functions for bgms Objects — extractor_functions","text":"functions extract various components objects returned `bgm()` function, edge indicators, posterior inclusion probabilities, parameter summaries. Internally, indicator samples stored `$gamma` (pre-0.1.4) `$indicator` (0.1.4–0.1.5). **bgms 0.1.6.0**, stored `$raw_samples$indicators`. Access via older names supported deprecated. Posterior inclusion probabilities computed edge indicators. Internally, indicator samples stored `$gamma` (pre-0.1.4) `$indicator` (0.1.4–0.1.5). **bgms 0.1.6.0**, stored `$raw_samples$indicator`. Access via older names supported deprecated. Category thresholds previously stored `$main_effects` (pre-0.1.4) `$posterior_mean_main` (0.1.4–0.1.5). **bgms 0.1.6.0**, stored `$posterior_summary_main`. Access via older names supported deprecated.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Extractor Functions for bgms Objects — extractor_functions","text":"- `extract_arguments()` – Extract model arguments - `extract_indicators()` – Get sampled edge indicators - `extract_posterior_inclusion_probabilities()` – Posterior edge inclusion probabilities - `extract_pairwise_interactions()` – Posterior mean pairwise interactions - `extract_category_thresholds()` – Posterior mean category thresholds - `extract_indicator_priors()` – Prior structure used edge indicators - `extract_sbm`  – Extract stochastic block model parameters (applicable)","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample observations from the ordinal MRF — mrfSampler","title":"Sample observations from the ordinal MRF — mrfSampler","text":"function samples states ordinal MRF using Gibbs sampler. Gibbs sampler initiated random values response options, proceeds simulating states variable logistic model using variable states predictor variables.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample observations from the ordinal MRF — mrfSampler","text":"","code":"mrfSampler(   no_states,   no_variables,   no_categories,   interactions,   thresholds,   variable_type = \"ordinal\",   reference_category,   iter = 1000 )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample observations from the ordinal MRF — mrfSampler","text":"no_states number states ordinal MRF generated. no_variables number variables ordinal MRF. no_categories Either positive integer vector positive integers length no_variables. number response categories top base category: no_categories = 1 generates binary states. interactions symmetric no_variables no_variables matrix pairwise interactions. -diagonal elements used. thresholds no_variables max(no_categories) matrix category thresholds. elements row indicate thresholds variable . no_categories vector, first no_categories[] elements used row . Blume-Capel model used category thresholds variable , row requires two values (details ); first \\(\\alpha\\), linear contribution Blume-Capel model second \\(\\beta\\), quadratic contribution. variable_type kind variables simulated? Can single character string specifying variable type p variables vector character strings length p specifying type variable separately. Currently, bgm supports “ordinal” “blume-capel”. Binary variables automatically treated “ordinal’’. Defaults variable_type = \"ordinal\". reference_category integer vector length no_variables specifying reference_category category used Blume-Capel model (details ). Can integer value 0 no_categories (no_categories[]). iter number iterations used Gibbs sampler. function provides last state Gibbs sampler output. default set 1e3.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample observations from the ordinal MRF — mrfSampler","text":"no_states no_variables matrix simulated states ordinal MRF.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sample observations from the ordinal MRF — mrfSampler","text":"two modeling options category thresholds. default option assumes category thresholds free, except first threshold set zero identification. user needs specify thresholds remaining response categories. option useful type ordinal variable gives user freedom specifying model. Blume-Capel option specifically designed ordinal variables special type reference_category category, neutral category Likert scale. Blume-Capel model specifies following quadratic model threshold parameters: $$\\mu_{\\text{c}} = \\alpha \\times \\text{c} + \\beta \\times (\\text{c} - \\text{r})^2,$$ \\(\\mu_{\\text{c}}\\) threshold category c (now includes zero), \\(\\alpha\\) offers linear trend across categories (increasing threshold values \\(\\alpha > 0\\) decreasing threshold values \\(\\alpha <0\\)), \\(\\beta < 0\\), offers increasing penalty responding category away reference_category category r, \\(\\beta > 0\\) suggests preference responding reference_category category.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample observations from the ordinal MRF — mrfSampler","text":"","code":"# Generate responses from a network of five binary and ordinal variables. no_variables = 5 no_categories = sample(1:5, size = no_variables, replace = TRUE)  Interactions = matrix(0, nrow = no_variables, ncol = no_variables) Interactions[2, 1] = Interactions[4, 1] = Interactions[3, 2] =   Interactions[5, 2] = Interactions[5, 4] = .25 Interactions = Interactions + t(Interactions) Thresholds = matrix(0, nrow = no_variables, ncol = max(no_categories))  x = mrfSampler(no_states = 1e3,                no_variables = no_variables,                no_categories = no_categories,                interactions = Interactions,                thresholds = Thresholds) #> Warning: The matrix ``thresholds'' contains numeric values for variable 1 for category  #> (categories, i.e., columns) exceding the maximum of 1. These values will  #> be ignored. #> Warning: The matrix ``thresholds'' contains numeric values for variable 5 for category  #> (categories, i.e., columns) exceding the maximum of 2. These values will  #> be ignored.  # Generate responses from a network of 2 ordinal and 3 Blume-Capel variables. no_variables = 5 no_categories = 4  Interactions = matrix(0, nrow = no_variables, ncol = no_variables) Interactions[2, 1] = Interactions[4, 1] = Interactions[3, 2] =   Interactions[5, 2] = Interactions[5, 4] = .25 Interactions = Interactions + t(Interactions)  Thresholds = matrix(NA, no_variables, no_categories) Thresholds[, 1] = -1 Thresholds[, 2] = -1 Thresholds[3, ] = sort(-abs(rnorm(4)), decreasing = TRUE) Thresholds[5, ] = sort(-abs(rnorm(4)), decreasing = TRUE)  x = mrfSampler(no_states = 1e3,                no_variables = no_variables,                no_categories = no_categories,                interactions = Interactions,                thresholds = Thresholds,                variable_type = c(\"b\",\"b\",\"o\",\"b\",\"o\"),                reference_category = 2)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for `bgmCompare` objects — print.bgmCompare","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"Minimal console output `bgmCompare` fit objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' print(x, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"x object class `bgmCompare`. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for `bgms` objects — print.bgms","title":"Print method for `bgms` objects — print.bgms","text":"Minimal console output `bgms` fit objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for `bgms` objects — print.bgms","text":"","code":"# S3 method for class 'bgms' print(x, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for `bgms` objects — print.bgms","text":"x object class `bgms`. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for `bgmCompare` objects — summary.bgmCompare","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"Returns posterior summaries diagnostics fitted `bgmCompare` model.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' summary(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"object object class `bgmCompare`. ... Currently ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"object class `summary.bgmCompare` posterior summaries.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for `bgms` objects — summary.bgms","title":"Summary method for `bgms` objects — summary.bgms","text":"Returns posterior summaries diagnostics fitted `bgms` model.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for `bgms` objects — summary.bgms","text":"","code":"# S3 method for class 'bgms' summary(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for `bgms` objects — summary.bgms","text":"object object class `bgms`. ... Currently ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary method for `bgms` objects — summary.bgms","text":"object class `summary.bgms` posterior summaries.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0161","dir":"Changelog","previous_headings":"","what":"bgms 0.1.6.1","title":"bgms 0.1.6.1","text":"CRAN release: 2025-10-04","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-6-1","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.6.1","text":"added extractor function joint SBM output cleaned documentation, c++ files changed length warmup phase warmup scheduler HMC / NUTS (15% → 7.5%)","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-6-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.6.1","text":"Fixed problem warmup scheduling adaptive-metropolis bgmCompare() Fixed stability problems parallel sampling bgm() Fixed spurious output errors printing console user interrupt.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0160","dir":"Changelog","previous_headings":"","what":"bgms 0.1.6.0","title":"bgms 0.1.6.0","text":"CRAN release: 2025-09-27","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-6-0","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.6.0","text":"added NUTS HMC options sampling bgm() bgmCompare() models added support running multiple chains parallel added user interrupt handling parallel sampling added Markov chain diagnostics (effective sample size R-hat) sampled parameters added summary(), print(), coef() methods fitted objects MCMC sampling bgm() bgmCompare() now reproducible seed argument specified","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-6-0","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.6.0","text":"improved progress bar parallel sampling summary() now integrates functionality old summary_SBM() removed options modeling main differences; main differences now always estimated selected, equivalent previous main_difference_model = \"collapse\" setting","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-6-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.6.0","text":"fixed --bounds error bgmCompare() handling missing data fixed bug SBM prior computation","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"deprecated-0-1-6-0","dir":"Changelog","previous_headings":"","what":"Deprecated","title":"bgms 0.1.6.0","text":"interaction_scale → use pairwise_scale burnin → use warmup save → longer needed (outputs returned default) threshold_alpha, threshold_beta → use main_alpha, main_beta main_difference_model (removed without replacement) reference_category → use baseline_category pairwise_difference_*, main_difference_* → use unified difference_* arguments pairwise_beta_bernoulli_*, main_beta_bernoulli_* → use unified beta_bernoulli_* arguments interaction_scale → use pairwise_scale threshold_alpha, threshold_beta → use main_alpha, main_beta burnin → use warmup save → longer needed extract_edge_indicators() → use extract_indicators() extract_pairwise_thresholds() → use extract_category_thresholds() $gamma (pre-0.1.4) $indicator (0.1.4–0.1.5) → replaced $raw_samples$indicator $main_effects (pre-0.1.4) $posterior_mean_main (0.1.4–0.1.5) → replaced $raw_samples$main (raw samples) $posterior_summary_main (summaries)","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-5-0","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.5.0 (GitHub only)","text":"bgmCompare function now allows network comparison two groups. new summary_sbm function can used summarize output bgm function “Stochastic-Block” prior. Two new data sets included package: ADHD Boredom.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-5-0","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.5.0 (GitHub only)","text":"bgm function “Stochastic-Block” prior can now also return sampled allocations block probabilities, sample return number blocks. underlying R c++ functions received massive update improve efficiency maintainance. Repository moved Bayesian Graphical Modelling Lab organization. Included custom c++ implementations exp log Windows.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-5-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.5.0 (GitHub only)","text":"Fixed bug bgmCompare function selecting group differences blume-capel parameters. Parameter differences selected fixed zero still updated. Fixed bug bgmCompare function handling samples blume-capel parameters. Output properly stored. Fixed bug bgmCompare function handling threshold estimation missing categories main_model = “Free”. sufficient statistics number categories computed correctly. Partially fixed bug bgms package slower Windows Linux MacOS. computation exp log using gcc compiler Windows really slow. custom c++ implementation, speed now closer speed achieved Linux MacOS.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0142","dir":"Changelog","previous_headings":"","what":"bgms 0.1.4.2","title":"bgms 0.1.4.2","text":"CRAN release: 2024-12-05","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-4-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.4.2","text":"fixed bug adjusting variance proposal distributions fixed bug recoding data “collapse” condition","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-4-2","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.4.2","text":"selection = TRUE, burnin phase now runs 2 * burnin iterations instead 1 * burnin. ensures chain starts well-calibrated parameter values changed maximum standard deviation adaptive proposal 20 back 2","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0141","dir":"Changelog","previous_headings":"","what":"bgms 0.1.4.1","title":"bgms 0.1.4.1","text":"CRAN release: 2024-11-12 minor release adds documentation output bug fixes.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-014","dir":"Changelog","previous_headings":"","what":"bgms 0.1.4","title":"bgms 0.1.4","text":"CRAN release: 2024-10-20","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-4","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.4","text":"Comparing category threshold pairwise interaction parameters two independent samples bgmCompare(). Stochastic Block model new prior option network structure bgm().","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-4","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.4","text":"Exported extractor functions extract results bgm objects safe way. Changed maximum standard deviation adaptive proposal 2 20. small bug fixes.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-013","dir":"Changelog","previous_headings":"","what":"bgms 0.1.3","title":"bgms 0.1.3","text":"CRAN release: 2024-02-25","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-3","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.3","text":"Added support Bayesian estimation without edge selection bgm(). Added support simulating data (mixed) binary, ordinal, Blume-Capel MRF mrfSampler() Added support analyzing (mixed) binary, ordinal, Blume-Capel variables bgm()","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"user-level-changes-0-1-3","dir":"Changelog","previous_headings":"","what":"User level changes","title":"bgms 0.1.3","text":"Removed support optimization based functions, mple(), mppe(), bgm.em() Removed support Unit-Information prior bgm() Removed support non-adaptive Metropolis bgm() Reduced file size saving raw MCMC samples","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-012","dir":"Changelog","previous_headings":"","what":"bgms 0.1.2","title":"bgms 0.1.2","text":"CRAN release: 2023-10-13 minor release adds bug fixes.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-011","dir":"Changelog","previous_headings":"","what":"bgms 0.1.1","title":"bgms 0.1.1","text":"CRAN release: 2023-09-01 minor release adding new features fixing minor bugs.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-1","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.1","text":"Missing data imputation bgm function. See na.action option. Prior distributions network structure bgm function. See edge_prior option. Adaptive Metropolis alternative current random walk Metropolis algorithm bgm function. See adaptive option.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"user-level-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"User level changes","title":"bgms 0.1.1","text":"Changed default specification interaction prior UnitInfo Cauchy. See interaction_prior option. Changed default threshold hyperparameter specification 1.0 0.5. See threshold_alpha threshold_beta options. Analysis output now uses column names data.","code":""}]
